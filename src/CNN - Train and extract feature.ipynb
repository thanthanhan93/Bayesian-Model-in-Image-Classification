{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network - Feature extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/800/0*V1vb9SDnsU1eZQUy.jpg\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet 5 </span></center>\n",
    "\n",
    "\n",
    "This LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6 **Activation.** ReLU **Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16. **Activation.** ReLU **Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs. **Activation.** ReLU\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs. **Activation.** ReLU\n",
    "\n",
    "**Layer 5: Fully Connected.** This should have 10 outputs. **Activation.** softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, mean=0, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='InputData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "class NeuralNetworkMNIST:\n",
    "    def __init__(self, LR=0.1, Batchsize=128, Iter=100, display_step = 1,\n",
    "                 Optimizer=tf.train.GradientDescentOptimizer, IsDropOut=False, Activation=tf.nn.sigmoid):\n",
    "        self.display_step = display_step \n",
    "        self.logs_path = \"log_files/\"\n",
    "        self.LR = LR\n",
    "        self.Iter = Iter\n",
    "        self.Batchsize = Batchsize\n",
    "        self.Optimizer = Optimizer\n",
    "        self.IsDropOut = IsDropOut\n",
    "        self.Activation = Activation\n",
    "\n",
    "            # Model\n",
    "        self.model = self.define_model(x, y)\n",
    "\n",
    "            # Minimize error using cross entropy\n",
    "        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.model, labels=y)\n",
    "        self.cost = tf.reduce_mean(self.cross_entropy)\n",
    "\n",
    "        # Gradient Descent\n",
    "        self.tfoptimizer = self.Optimizer(self.LR).minimize(self.cost)\n",
    "\n",
    "        # Accuracy\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.model, 1), tf.argmax(y, 1))\n",
    "        self.accuracy_operation = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        self.init = tf.global_variables_initializer()\n",
    "    \n",
    "    def define_model(self, x, y):\n",
    "        mu = 0\n",
    "        sigma = 0.1\n",
    "        with tf.name_scope(\"Layer_1\"):\n",
    "        # resized image input 28x28x1 => 32x32x1\n",
    "            x_new = tf.map_fn(lambda x1: tf.image.pad_to_bounding_box(x1, 2, 2, 32, 32), x)\n",
    "\n",
    "            # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "            conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "            conv1_b = bias_variable([6])\n",
    "            conv1_pre   = tf.nn.conv2d(x_new, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "            # SOLUTION: Activation.\n",
    "            conv1_ac = self.Activation(conv1_pre)\n",
    "\n",
    "            # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "            conv1 = tf.nn.max_pool(conv1_ac, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "        with tf.name_scope(\"Layer_2\"):\n",
    "            conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "            conv2_b = bias_variable([16])\n",
    "            conv2_pre   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "            # SOLUTION: Activation.\n",
    "            conv2_ac = self.Activation(conv2_pre)\n",
    "\n",
    "            # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "            conv2 = tf.nn.max_pool(conv2_ac, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # SOLUTION: Flatten. Input = 5x5x16. Output = 400.t multiple values for keyword argument 'LR\n",
    "        with tf.name_scope(\"Flatten\"):\n",
    "            fc0   = flatten(conv2)\n",
    "        \n",
    "        # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "        with tf.name_scope(\"Layer_3\"):\n",
    "            fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "            fc1_b = tf.Variable(tf.zeros(120))\n",
    "            fc1_pre   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "\n",
    "            # SOLUTION: Activation.\n",
    "            fc1   = self.Activation(fc1_pre)\n",
    "        \n",
    "        fc1_out = None\n",
    "        # keep probability number to be input in training\n",
    "        # drop out\n",
    "        if (self.IsDropOut):\n",
    "            with tf.name_scope(\"Dropout_1\"):\n",
    "                fc1_out = tf.nn.dropout(fc1, keep_prob)\n",
    "        else:\n",
    "            fc1_out = fc1\n",
    "\n",
    "        # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        with tf.name_scope(\"Layer_4\"):\n",
    "            fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "            fc2_b  = bias_variable([84])\n",
    "            fc2_pre = tf.matmul(fc1_out, fc2_W) + fc2_b\n",
    "\n",
    "        # SOLUTION: Activation.\n",
    "            fc2    = self.Activation(fc2_pre)\n",
    "        \n",
    "        self.fc2_out = None\n",
    "        if (self.IsDropOut):\n",
    "            with tf.name_scope(\"Dropout_2\"):\n",
    "                self.fc2_out = tf.nn.dropout(fc2, keep_prob)\n",
    "        else:\n",
    "            self.fc2_out = fc2\n",
    "            \n",
    "        # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "        with tf.name_scope(\"Layer_5\"):\n",
    "            fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "            fc3_b  = bias_variable([10])\n",
    "            fc3_pre = tf.matmul(self.fc2_out, fc3_W) + fc3_b\n",
    "\n",
    "            logits = fc3_pre\n",
    "        return logits\n",
    "      \n",
    "    def extractFeature(self,X_val):\n",
    "        X_val_reshaped = X_val.reshape([-1, 28, 28, 1])\n",
    "        if (self.IsDropOut):\n",
    "            return self.sess.run([self.fc2_out], feed_dict={x: X_val_reshaped, keep_prob: 1.0})\n",
    "        else:\n",
    "            return self.sess.run([self.fc2_out], feed_dict={x: X_val_reshaped})\n",
    "    \n",
    "    def evaluation(self, X_val, y_val, sess):\n",
    "        X_val_reshaped = X_val.reshape([-1, 28, 28, 1])\n",
    "        accuracy = 0\n",
    "        if (self.IsDropOut):\n",
    "            accuracy = sess.run([self.accuracy_operation], feed_dict={x: X_val_reshaped, y: y_val, keep_prob: 1.0})\n",
    "        else:\n",
    "            accuracy = sess.run([self.accuracy_operation], feed_dict={x: X_val_reshaped, y: y_val})\n",
    "        return accuracy\n",
    "        \n",
    "    def train(self, X_train, y_train, X_validation, y_validation):\n",
    "        # Initializing the session \n",
    "        print (\"Start Training!\")\n",
    "        self.sess = tf.Session()\n",
    "        sess = self.sess\n",
    "        sess.close()\n",
    "        sess.run(self.init)\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(self.logs_path, graph=tf.get_default_graph())\n",
    "        # Training cycle\n",
    "        start = time.time()\n",
    "        for epoch in np.arange(self.Iter):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/self.Batchsize)\n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "            # Loop over all batches\n",
    "            for offset in range(0, mnist.train.num_examples, self.Batchsize):\n",
    "                end = offset + self.Batchsize\n",
    "                batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "                batch_x = batch_x.reshape([-1,28,28,1])\n",
    "\n",
    "                c = None\n",
    "                if (self.IsDropOut == False):\n",
    "                    _, c = sess.run([self.tfoptimizer, self.cost], \n",
    "                                             feed_dict={x: batch_x, y: batch_y})\n",
    "                else:\n",
    "                    _, c = sess.run([self.tfoptimizer, self.cost], \n",
    "                                             feed_dict={x: batch_x, y: batch_y, keep_prob: 0.75})\n",
    "                avg_cost += c / total_batch\n",
    "\n",
    "            if (epoch+1) % self.display_step == 0:\n",
    "                accu = self.evaluation(X_validation, y_validation, sess)\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \n",
    "                      \", Loss=\", \"{:.9f}\".format(avg_cost), \n",
    "                      \", Validation accuracy=\", accu\n",
    "                     )\n",
    "            else:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \n",
    "                      \", Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "                    \n",
    "        print(\"Optimization Finished! in %f seconds\"%(time.time()-start))\n",
    "        # Save model\n",
    "        self.saver.save(sess, \"mnist_nn_\" + str(self.Iter) + \"_\" + str(time.time()), global_step=100)\n",
    "        # Test model - Calculate accuracy\n",
    "        print(\"Accuracy on test data: \", self.evaluation(mnist.test.images, mnist.test.labels, sess))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01 , Loss= 0.459923559\n",
      "Epoch:  02 , Loss= 0.134445831\n",
      "Epoch:  03 , Loss= 0.096902787\n",
      "Epoch:  04 , Loss= 0.074125227\n",
      "Epoch:  05 , Loss= 0.065016546 , Validation accuracy= [0.98559999]\n",
      "Epoch:  06 , Loss= 0.058340095\n",
      "Epoch:  07 , Loss= 0.050417239\n",
      "Epoch:  08 , Loss= 0.046235921\n",
      "Epoch:  09 , Loss= 0.041521387\n",
      "Epoch:  10 , Loss= 0.039693081 , Validation accuracy= [0.99040002]\n",
      "Epoch:  11 , Loss= 0.034952509\n",
      "Epoch:  12 , Loss= 0.032905150\n",
      "Epoch:  13 , Loss= 0.029796152\n",
      "Epoch:  14 , Loss= 0.027261079\n",
      "Epoch:  15 , Loss= 0.027337105 , Validation accuracy= [0.99000001]\n",
      "Epoch:  16 , Loss= 0.023096184\n",
      "Epoch:  17 , Loss= 0.021861705\n",
      "Epoch:  18 , Loss= 0.020957493\n",
      "Epoch:  19 , Loss= 0.019923194\n",
      "Epoch:  20 , Loss= 0.018759270 , Validation accuracy= [0.99299997]\n",
      "Epoch:  21 , Loss= 0.018844684\n",
      "Epoch:  22 , Loss= 0.016890804\n",
      "Epoch:  23 , Loss= 0.018270153\n",
      "Epoch:  24 , Loss= 0.016669049\n",
      "Epoch:  25 , Loss= 0.015644376 , Validation accuracy= [0.99000001]\n",
      "Epoch:  26 , Loss= 0.015762045\n",
      "Epoch:  27 , Loss= 0.013635977\n",
      "Epoch:  28 , Loss= 0.014601333\n",
      "Epoch:  29 , Loss= 0.012991552\n",
      "Epoch:  30 , Loss= 0.011890239 , Validation accuracy= [0.9896]\n",
      "Epoch:  31 , Loss= 0.012589186\n",
      "Epoch:  32 , Loss= 0.009370331\n",
      "Epoch:  33 , Loss= 0.011217023\n",
      "Epoch:  34 , Loss= 0.012959585\n",
      "Epoch:  35 , Loss= 0.011934360 , Validation accuracy= [0.99260002]\n",
      "Epoch:  36 , Loss= 0.009842084\n",
      "Epoch:  37 , Loss= 0.010016957\n",
      "Epoch:  38 , Loss= 0.009835038\n",
      "Epoch:  39 , Loss= 0.010441038\n",
      "Epoch:  40 , Loss= 0.009820574 , Validation accuracy= [0.991]\n",
      "Epoch:  41 , Loss= 0.009733305\n",
      "Epoch:  42 , Loss= 0.009756342\n",
      "Epoch:  43 , Loss= 0.009075672\n",
      "Epoch:  44 , Loss= 0.009065317\n",
      "Epoch:  45 , Loss= 0.009603547 , Validation accuracy= [0.99299997]\n",
      "Epoch:  46 , Loss= 0.009489307\n",
      "Epoch:  47 , Loss= 0.008078057\n",
      "Epoch:  48 , Loss= 0.008766112\n",
      "Epoch:  49 , Loss= 0.007077216\n",
      "Epoch:  50 , Loss= 0.007857900 , Validation accuracy= [0.99180001]\n",
      "Epoch:  51 , Loss= 0.009146653\n",
      "Epoch:  52 , Loss= 0.008954532\n",
      "Epoch:  53 , Loss= 0.007907816\n",
      "Epoch:  54 , Loss= 0.007438715\n",
      "Epoch:  55 , Loss= 0.006348660 , Validation accuracy= [0.99180001]\n",
      "Epoch:  56 , Loss= 0.007657963\n",
      "Epoch:  57 , Loss= 0.007990878\n",
      "Epoch:  58 , Loss= 0.006358707\n",
      "Epoch:  59 , Loss= 0.008243955\n",
      "Epoch:  60 , Loss= 0.006170526 , Validation accuracy= [0.99199998]\n",
      "Epoch:  61 , Loss= 0.007117917\n",
      "Epoch:  62 , Loss= 0.006126920\n",
      "Epoch:  63 , Loss= 0.007408895\n",
      "Epoch:  64 , Loss= 0.007288476\n",
      "Epoch:  65 , Loss= 0.006506382 , Validation accuracy= [0.99059999]\n",
      "Epoch:  66 , Loss= 0.006535044\n",
      "Epoch:  67 , Loss= 0.006868177\n",
      "Epoch:  68 , Loss= 0.004300267\n",
      "Epoch:  69 , Loss= 0.005947306\n",
      "Epoch:  70 , Loss= 0.006626243 , Validation accuracy= [0.99239999]\n",
      "Epoch:  71 , Loss= 0.005653097\n",
      "Epoch:  72 , Loss= 0.006911764\n",
      "Epoch:  73 , Loss= 0.006666891\n",
      "Epoch:  74 , Loss= 0.005973723\n",
      "Epoch:  75 , Loss= 0.005985562 , Validation accuracy= [0.99260002]\n",
      "Epoch:  76 , Loss= 0.007474634\n",
      "Epoch:  77 , Loss= 0.004299512\n",
      "Epoch:  78 , Loss= 0.004437524\n",
      "Epoch:  79 , Loss= 0.004201469\n",
      "Epoch:  80 , Loss= 0.007250312 , Validation accuracy= [0.99239999]\n",
      "Epoch:  81 , Loss= 0.005143389\n",
      "Epoch:  82 , Loss= 0.006838435\n",
      "Epoch:  83 , Loss= 0.005068375\n",
      "Epoch:  84 , Loss= 0.005103394\n",
      "Epoch:  85 , Loss= 0.006429682 , Validation accuracy= [0.99260002]\n",
      "Epoch:  86 , Loss= 0.006677250\n",
      "Epoch:  87 , Loss= 0.005759480\n",
      "Epoch:  88 , Loss= 0.005472420\n",
      "Epoch:  89 , Loss= 0.005121010\n",
      "Epoch:  90 , Loss= 0.005145212 , Validation accuracy= [0.99220002]\n",
      "Epoch:  91 , Loss= 0.004797350\n",
      "Epoch:  92 , Loss= 0.006776835\n",
      "Epoch:  93 , Loss= 0.004548381\n",
      "Epoch:  94 , Loss= 0.006073237\n",
      "Epoch:  95 , Loss= 0.004066456 , Validation accuracy= [0.991]\n",
      "Epoch:  96 , Loss= 0.004816023\n",
      "Epoch:  97 , Loss= 0.005394668\n",
      "Epoch:  98 , Loss= 0.004632601\n",
      "Epoch:  99 , Loss= 0.005752317\n",
      "Epoch:  100 , Loss= 0.004117261 , Validation accuracy= [0.99260002]\n",
      "Optimization Finished! in 2321.097617 seconds\n",
      "Accuracy on test data:  [0.99150002]\n"
     ]
    }
   ],
   "source": [
    "mnist_nn = NeuralNetworkMNIST(LR=0.001, Iter=100, display_step=5,\n",
    "                         Optimizer=tf.train.AdamOptimizer, IsDropOut=True, Activation=tf.nn.relu)\n",
    "mnist_nn.train(X_train, y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = np.concatenate([X_train, X_validation], axis=0)\n",
    "y_full = np.concatenate([y_train, y_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans_train = mnist_nn.extractFeature(X_full)\n",
    "X_trans_test = mnist_nn.extractFeature(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('minist_x_train.npy', X_trans_train[0])\n",
    "np.save('minist_y_train.npy', y_full)\n",
    "np.save('minist_x_test.npy', X_trans_test[0])\n",
    "np.save('minist_y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 1], name='InputData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "class NeuralNetworkCIFAR:\n",
    "    def __init__(self, LR=0.1, Batchsize=128, Iter=100, display_step = 1,\n",
    "                 Optimizer=tf.train.GradientDescentOptimizer, IsDropOut=False, Activation=tf.nn.sigmoid):\n",
    "        self.display_step = display_step \n",
    "        self.logs_path = \"log_files/\"\n",
    "        self.LR = LR\n",
    "        self.Iter = Iter\n",
    "        self.Batchsize = Batchsize\n",
    "        self.Optimizer = Optimizer\n",
    "        self.IsDropOut = IsDropOut\n",
    "        self.Activation = Activation\n",
    "\n",
    "            # Model\n",
    "        self.model = self.define_model(x, y)\n",
    "\n",
    "            # Minimize error using cross entropy\n",
    "        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.model, labels=y)\n",
    "        self.cost = tf.reduce_mean(self.cross_entropy)\n",
    "\n",
    "        # Gradient Descent\n",
    "        self.tfoptimizer = self.Optimizer(self.LR).minimize(self.cost)\n",
    "\n",
    "        # Accuracy\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.model, 1), tf.argmax(y, 1))\n",
    "        self.accuracy_operation = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        self.init = tf.global_variables_initializer()\n",
    "    \n",
    "    def define_model(self, x, y):\n",
    "        mu = 0\n",
    "        sigma = 0.1\n",
    "        with tf.name_scope(\"Layer_1\"):\n",
    "        # resized image input 28x28x1 => 32x32x1\n",
    "            x_new = tf.map_fn(lambda x1: tf.image.pad_to_bounding_box(x1, 2, 2, 32, 32), x)\n",
    "\n",
    "            # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "            conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "            conv1_b = bias_variable([6])\n",
    "            conv1_pre   = tf.nn.conv2d(x_new, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "            # SOLUTION: Activation.\n",
    "            conv1_ac = self.Activation(conv1_pre)\n",
    "\n",
    "            # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "            conv1 = tf.nn.max_pool(conv1_ac, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "        with tf.name_scope(\"Layer_2\"):\n",
    "            conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "            conv2_b = bias_variable([16])\n",
    "            conv2_pre   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "            # SOLUTION: Activation.\n",
    "            conv2_ac = self.Activation(conv2_pre)\n",
    "\n",
    "            # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "            conv2 = tf.nn.max_pool(conv2_ac, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # SOLUTION: Flatten. Input = 5x5x16. Output = 400.t multiple values for keyword argument 'LR\n",
    "        with tf.name_scope(\"Flatten\"):\n",
    "            fc0   = flatten(conv2)\n",
    "        \n",
    "        # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "        with tf.name_scope(\"Layer_3\"):\n",
    "            fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "            fc1_b = tf.Variable(tf.zeros(120))\n",
    "            fc1_pre   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "\n",
    "            # SOLUTION: Activation.\n",
    "            fc1   = self.Activation(fc1_pre)\n",
    "        \n",
    "        fc1_out = None\n",
    "        # keep probability number to be input in training\n",
    "        # drop out\n",
    "        if (self.IsDropOut):\n",
    "            with tf.name_scope(\"Dropout_1\"):\n",
    "                fc1_out = tf.nn.dropout(fc1, keep_prob)\n",
    "        else:\n",
    "            fc1_out = fc1\n",
    "\n",
    "        # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        with tf.name_scope(\"Layer_4\"):\n",
    "            fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "            fc2_b  = bias_variable([84])\n",
    "            fc2_pre = tf.matmul(fc1_out, fc2_W) + fc2_b\n",
    "\n",
    "        # SOLUTION: Activation.\n",
    "            fc2    = self.Activation(fc2_pre)\n",
    "        \n",
    "        self.fc2_out = None\n",
    "        if (self.IsDropOut):\n",
    "            with tf.name_scope(\"Dropout_2\"):\n",
    "                self.fc2_out = tf.nn.dropout(fc2, keep_prob)\n",
    "        else:\n",
    "            self.fc2_out = fc2\n",
    "            \n",
    "        # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "        with tf.name_scope(\"Layer_5\"):\n",
    "            fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "            fc3_b  = bias_variable([10])\n",
    "            fc3_pre = tf.matmul(self.fc2_out, fc3_W) + fc3_b\n",
    "\n",
    "            logits = fc3_pre\n",
    "        return logits\n",
    "      \n",
    "    def extractFeature(self,X_val):\n",
    "        X_val_reshaped = X_val.reshape([-1, 28, 28, 1])\n",
    "        if (self.IsDropOut):\n",
    "            return self.sess.run([self.fc2_out], feed_dict={x: X_val_reshaped, keep_prob: 1.0})\n",
    "        else:\n",
    "            return self.sess.run([self.fc2_out], feed_dict={x: X_val_reshaped})\n",
    "    \n",
    "    def evaluation(self, X_val, y_val, sess):\n",
    "        X_val_reshaped = X_val.reshape([-1, 28, 28, 1])\n",
    "        accuracy = 0\n",
    "        if (self.IsDropOut):\n",
    "            accuracy = sess.run([self.accuracy_operation], feed_dict={x: X_val_reshaped, y: y_val, keep_prob: 1.0})\n",
    "        else:\n",
    "            accuracy = sess.run([self.accuracy_operation], feed_dict={x: X_val_reshaped, y: y_val})\n",
    "        return accuracy\n",
    "        \n",
    "    def train(self, X_train, y_train, X_validation, y_validation):\n",
    "        # Initializing the session \n",
    "        print (\"Start Training!\")\n",
    "        self.sess = tf.Session()\n",
    "        sess = self.sess\n",
    "        sess.close()\n",
    "        sess.run(self.init)\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(self.logs_path, graph=tf.get_default_graph())\n",
    "        # Training cycle\n",
    "        start = time.time()\n",
    "        for epoch in np.arange(self.Iter):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/self.Batchsize)\n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "            # Loop over all batches\n",
    "            for offset in range(0, mnist.train.num_examples, self.Batchsize):\n",
    "                end = offset + self.Batchsize\n",
    "                batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "                batch_x = batch_x.reshape([-1,28,28,1])\n",
    "\n",
    "                c = None\n",
    "                if (self.IsDropOut == False):\n",
    "                    _, c = sess.run([self.tfoptimizer, self.cost], \n",
    "                                             feed_dict={x: batch_x, y: batch_y})\n",
    "                else:\n",
    "                    _, c = sess.run([self.tfoptimizer, self.cost], \n",
    "                                             feed_dict={x: batch_x, y: batch_y, keep_prob: 0.75})\n",
    "                avg_cost += c / total_batch\n",
    "\n",
    "            if (epoch+1) % self.display_step == 0:\n",
    "                accu = self.evaluation(X_validation, y_validation, sess)\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \n",
    "                      \", Loss=\", \"{:.9f}\".format(avg_cost), \n",
    "                      \", Validation accuracy=\", accu\n",
    "                     )\n",
    "            else:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \n",
    "                      \", Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "                    \n",
    "        print(\"Optimization Finished! in %f seconds\"%(time.time()-start))\n",
    "        # Save model\n",
    "        self.saver.save(sess, \"mnist_nn_\" + str(self.Iter) + \"_\" + str(time.time()), global_step=100)\n",
    "        # Test model - Calculate accuracy\n",
    "        print(\"Accuracy on test data: \", self.evaluation(mnist.test.images, mnist.test.labels, sess))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
