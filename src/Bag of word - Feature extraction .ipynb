{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of word model - Feature extraction\n",
    "\n",
    "<br>\n",
    "**DISCLAIMER: IT TAKES LOTS OF TIME TO RUNING THIS NOTEBOOK. IT IS BETTER TO USE THE GENERATED CODEBOOK AND TRANSFORMED FEATURE AS THE RESULT OF THIS MODEL WHICH YOU CAN DOWNLOAD IN THE THIS FOLDER: https://drive.google.com/drive/folders/0Bxk-xCNz8VClZEs5YVFoV3MyZEE?usp=sharing. THE DETAIL WILL BE MENTIONED IN THIS NOTEBOOK**\n",
    "\n",
    "<br>\n",
    "In this notebook, I just want to show you again the process of the BoW model and how can we represent the image by it. In document classification, a bag of words is a sparse vector of occurrence counts of words; that is, a sparse histogram over the vocabulary. In computer vision, a bag of visual words is a vector of occurrence counts of a vocabulary of local image features. Two most important steps are feature extraction using SIFT and codebook generation using K-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:09:13.383620Z",
     "start_time": "2017-06-03T13:09:09.962851Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse as ap\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "from scipy.cluster.vq import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import skimage\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1. Obtain training data *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:09:17.489700Z",
     "start_time": "2017-06-03T13:09:15.545744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the training classes names and store them in a list\n",
    "train_path = \"data/CifarTrain/\"\n",
    "training_names = os.listdir(train_path)\n",
    "\n",
    "# image_paths and the corresponding label in image_paths\n",
    "image_paths = []\n",
    "image_classes = []\n",
    "class_id = -1\n",
    "for training_name in training_names:\n",
    "    class_path_url = os.path.join(train_path, training_name)\n",
    "    from os import walk\n",
    "\n",
    "    class_path = []\n",
    "    for (dirpath, dirnames, filenames) in walk(class_path_url):\n",
    "        for fn in filenames:\n",
    "            class_path.append(os.path.join(class_path_url,fn))\n",
    "        break\n",
    "    \n",
    "    image_paths+=class_path\n",
    "    image_classes+=[class_id]*len(class_path)\n",
    "    class_id+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2. Extract feature points and its description *\n",
    "In this model, I use SIFT as detectors and descriptors which is the most popular method in CV. There are lots of descriptor and detector such as FAST, ORB, SURF. Even SIFT computation is quite expensive and costly but the performance is quite good in feature extraction. In this step, I use SIFT to collect all features in training set to prepare input for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T18:04:01.408772Z",
     "start_time": "2017-06-01T18:02:44.143073Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create feature extraction and keypoint detector objects\n",
    "sift = cv2.SIFT()\n",
    "# List where all the descriptors are stored\n",
    "des_list = []\n",
    "i = 0 \n",
    "for image_path in image_paths:\n",
    "    if (i%1000==0):\n",
    "        print (i)\n",
    "    i += 1\n",
    "    # read image\n",
    "    im = cv2.imread(image_path)\n",
    "    \n",
    "    # find the keypoints with SIFT\n",
    "    kp = sift.detect(im,None)\n",
    "    # compute the descriptors with SIFT\n",
    "    kp, des = sift.compute(im, kp)\n",
    "    \n",
    "    des_list.append(des)   \n",
    "    \n",
    "# Remove some empty list\n",
    "des_list_ = [x for x in des_list if np.shape(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T18:12:53.355975Z",
     "start_time": "2017-06-01T18:12:53.143929Z"
    }
   },
   "outputs": [],
   "source": [
    "descriptors = np.concatenate(des_list_,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T18:21:53.323823Z",
     "start_time": "2017-06-01T18:21:52.410260Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Stack all the descriptors vertically in a numpy array\n",
    "np.save('features/train_feature_n',descriptors)\n",
    "del des_list_,des_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *3. Perform k-means clustering to train the codebook *\n",
    "Because of huge number of features and slight difference between them so that we should use clustering to group all similiar features together. The output the clustering is dictionary of centroids, called codebook. Number of words in the codebook should be high (10.000 words). Because of long training time, I just use 1000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:09:25.357472Z",
     "start_time": "2017-06-03T13:09:24.757549Z"
    }
   },
   "outputs": [],
   "source": [
    "descriptors = np.load('features/train_feature_n.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:12:53.125205Z",
     "start_time": "2017-06-03T13:09:35.225660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform k-means clustering\n",
    "k = 100\n",
    "voc, variance = kmeans(descriptors, k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:12:53.194026Z",
     "start_time": "2017-06-03T13:12:53.190277Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('codebook_100',voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *4. Represent training data with BoW *\n",
    "After we have the codebook, we will represent our image again. The new vector has a length equal to number of words in the codebook and the value is number of occurences of those features in a image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:22:48.433035Z",
     "start_time": "2017-06-03T13:22:48.428818Z"
    }
   },
   "outputs": [],
   "source": [
    "voc = np.load('codebook_100.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:25:20.148342Z",
     "start_time": "2017-06-03T13:22:54.933668Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nyr/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the histogram of features\n",
    "sift = cv2.SIFT()\n",
    "train_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "for i in xrange(len(image_paths)):\n",
    "    im = cv2.imread(image_paths[i])\n",
    "    kp = sift.detect(im,None)\n",
    "    kp, des = sift.compute(im, kp)\n",
    "    if(des!=None):\n",
    "        words, distance = vq(des,voc)\n",
    "        for w in words:\n",
    "            train_features[i][w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:25:35.295899Z",
     "start_time": "2017-06-03T13:25:35.286372Z"
    }
   },
   "outputs": [],
   "source": [
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:25:37.876335Z",
     "start_time": "2017-06-03T13:25:37.291336Z"
    }
   },
   "outputs": [],
   "source": [
    "test_path = \"cifarTest/\"\n",
    "image_paths_test = list(paths.list_images(test_path))\n",
    "x = [i[10:][:-4] for i in image_paths_test]\n",
    "testClass = [int(i[-1]) for i in x]\n",
    "idxTest = np.argsort([int(i[:-2]) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:26:11.011078Z",
     "start_time": "2017-06-03T13:25:43.189943Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nyr/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:14: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "# Get the training classes names and store them in a list\n",
    "#test_path = \"data/cifarTest/\"\n",
    "#test_path = \"cifarTest/\"\n",
    "\n",
    "#image_paths_test = list(paths.list_images(test_path))\n",
    "#idxTest = np.argsort([int(i[15:][:-4]) for i in image_paths_test])\n",
    "image_paths_test = np.array(image_paths_test)[idxTest]\n",
    "# Calculate the histogram of features\n",
    "test_features = np.zeros((len(image_paths_test), k), \"float32\")\n",
    "for i in xrange(len(image_paths_test)):\n",
    "    im = cv2.imread(image_paths_test[i])\n",
    "    kp = sift.detect(im,None)\n",
    "    kp, des = sift.compute(im, kp)\n",
    "    if(des!=None):\n",
    "        words, distance = vq(des,voc)\n",
    "        for w in words:\n",
    "            test_features[i][w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *5. Apply tf-idf weighting for representation *\n",
    "The idea is strengthen useful words which is discrimative and lighten the common words. In tf-idf, term frequency grasp the idea that if the word appears many times in the documents that means it can be \"representive\" word for this document and Inverse document frequency means if the word appears in almosts documents that will be useless to use it to classify the document and vice versa. There are many variance for tf and idf weighting. In this case, i implement 4 ways to weight tf:\n",
    "  1. RAW: occurance of word in document \n",
    "  2. Frequency: normalize with total number of words in a document\n",
    "  3. log normalization: reduce the high difference between 2 features\n",
    "  4. double normalization: normalize by using maximum value in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:26:11.131562Z",
     "start_time": "2017-06-03T13:26:11.119215Z"
    }
   },
   "outputs": [],
   "source": [
    "def tf_transform(data,method=''):\n",
    "    if (method == 'frequency'):\n",
    "        wordfrequencyDocs = np.sum(data , axis = 1) + 1e-10\n",
    "        return (data.T/wordfrequencyDocs).T\n",
    "    if (method == 'log'):\n",
    "        data[data != 0] = 1 + np.log(data[data != 0])\n",
    "        return data\n",
    "    if (method == 'doublenorm'):\n",
    "        mostfrequencyWord = np.max(data , axis = 1) + 1e-10\n",
    "        return (data.T/mostfrequencyWord).T\n",
    "    # raw count approach\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:26:11.280648Z",
     "start_time": "2017-06-03T13:26:11.230585Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Perform Tf-Idf vectorization\n",
    "nbr_occurences = np.sum((train_features > 0) * 1, axis = 0)\n",
    "idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *6. Save new vectors of feature for next step*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:26:11.674811Z",
     "start_time": "2017-06-03T13:26:11.372728Z"
    }
   },
   "outputs": [],
   "source": [
    "name_2 = '_100'\n",
    "train_TfIdf = idf*tf_transform(train_features,'frequency')\n",
    "np.save('features/BoW_train_frequency'+name_2,train_TfIdf)\n",
    "train_TfIdf = idf*tf_transform(train_features,'doublenorm')\n",
    "np.save('features/BoW_train_doublenorm'+name_2,train_TfIdf)\n",
    "train_TfIdf = idf*tf_transform(train_features,'log')\n",
    "np.save('features/BoW_train_log'+name_2,train_TfIdf)\n",
    "train_TfIdf = idf*tf_transform(train_features,'')\n",
    "np.save('features/BoW_train_raw'+name_2,train_TfIdf)\n",
    "np.save('features/BoW_train _labels',image_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T13:26:11.865255Z",
     "start_time": "2017-06-03T13:26:11.777098Z"
    }
   },
   "outputs": [],
   "source": [
    "test_TfIdf = idf*tf_transform(test_features,'frequency')\n",
    "np.save('features/BoW_test_frequency' +name_2,test_TfIdf)\n",
    "test_TfIdf = idf*tf_transform(test_features,'doublenorm')\n",
    "np.save('features/BoW_test_doublenorm' +name_2,test_TfIdf)\n",
    "test_TfIdf = idf*tf_transform(test_features,'log')\n",
    "np.save('features/BoW_test_log' +name_2,test_TfIdf)\n",
    "test_TfIdf = idf*tf_transform(test_features,'')\n",
    "np.save('features/BoW_test_raw' +name_2,test_TfIdf)\n",
    "np.save('features/Bow_test_labels',testClass)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 246,
   "position": {
    "height": "268px",
    "left": "1322px",
    "right": "20px",
    "top": "13px",
    "width": "317px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
